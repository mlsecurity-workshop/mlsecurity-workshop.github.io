[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "",
    "text": "Important Information\n\nDate: September 17th, 2025\n\nTime: 9h30 - 17h00\n\nLocation: Esclangon building, 1st floor, Campus Pierre et Marie Curie, 4 place Jussieu, 75005 Paris\nLanguage: English\nRegistration: Mandatory (Limited places)\nCost: Free\n\n\n\n\nThis workshop focuses on adversarial and backdoor attacks targeting real-life machine learning systems. We will explore vulnerabilities in deployed learning systems, examine attack vectors in practical scenarios, and discuss defense mechanisms for robust ML deployment. The workshop is inspired by research from the KINAITICS project, which investigates kinematic indicators for adversarial behavior detection in AI systems. The event brings together researchers, academics, and industry professionals to discuss cutting-edge developments in adversarial machine learning, security implications, and mitigation strategies for production environments.\n\n\n\nRafaël Pinot (Sorbonne University) and Cédric Gouy-Pailler (CEA)"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "",
    "text": "Important Information\n\nDate: September 17th, 2025\n\nTime: 9h30 - 17h00\n\nLocation: Esclangon building, 1st floor, Campus Pierre et Marie Curie, 4 place Jussieu, 75005 Paris\nLanguage: English\nRegistration: Mandatory (Limited places)\nCost: Free\n\n\n\n\nThis workshop focuses on adversarial and backdoor attacks targeting real-life machine learning systems. We will explore vulnerabilities in deployed learning systems, examine attack vectors in practical scenarios, and discuss defense mechanisms for robust ML deployment. The workshop is inspired by research from the KINAITICS project, which investigates kinematic indicators for adversarial behavior detection in AI systems. The event brings together researchers, academics, and industry professionals to discuss cutting-edge developments in adversarial machine learning, security implications, and mitigation strategies for production environments.\n\n\n\nRafaël Pinot (Sorbonne University) and Cédric Gouy-Pailler (CEA)"
  },
  {
    "objectID": "index.html#program",
    "href": "index.html#program",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "Program",
    "text": "Program\n\nWorkshop Agenda\nSeptember 17th, 2025\n\n\n\n\n\n\n\n\nTime\nSession\nSpeaker/Activity\n\n\n\n\n9h30 - 9h45\nRegistration & Welcome Coffee\n\n\n\n9h45 - 10h00\nOpening Remarks\nRafaël Pinot & Cédric Gouy-Pailler\n\n\n10h00 - 11h00\nKeynote 1: Adversarial attacks and mitigations\nBenjamin Negrevergne\n\n\n11h00 - 11h20\nCoffee Break\n\n\n\n11h20 - 12h15\nSession 1: Real-world Attack Scenarios\n[Speaker(s)]\n\n\n12h15 - 13h45\nLunch Break\n\n\n\n13h45 - 14h45\nKeynote 2: Backdoors in Artificial Intelligence: Stealth Weapon or Structural Weakness?\nKassem Kallas\n\n\n14h45 - 15h30\nSession 2: Defense Mechanisms & Mitigation\n[Speaker(s)]\n\n\n15h30 - 15h45\nCoffee Break\n\n\n\n15h45 - 16h30\nSession 3: Industry Case Studies\n[Speaker(s)]\n\n\n16h30 - 17h00\nClosing Remarks & Networking\nOrganizers\n\n\n\nProgram subject to modifications\n\n\nKeynote Speakers\n\n\n\n  \n    \n    \n      Dr. Benjamin NEGREVERGNE\n      LAMSADE, PSL-Paris Dauphine University\n      Associate Professor\n    \n  \n  Keynote 1: Adversarial attacks and mitigations\n  Abstract: Adversarial machine learning no longer centers on imperceptible pixel tweaks. \n  As foundation models become multimodal and instruction-tuned, the attack surface shifts to safety \n  alignment itself and to the prompts that govern multi-turn reasoning. This talk frames these trends—adversarial \n  alignment and prompt-level manipulation—using recent studies on large language and vision-language systems as \n  touchstones, and outlines why future defenses must address both model internals and their interactive context.\n\n\n\n  \n    \n    \n      Dr. Kassem KALLAS\n      INSERM, IMT Atlantique\n      Senior Scientist, HDR\n    \n  \n  Keynote 2: Backdoors in Artificial Intelligence: Stealth Weapon or Structural Weakness?\n  Abstract: Backdoor attacks represent a stealthy yet serious threat to the \n  integrity of AI systems, especially in black-box image classification settings. This talk will begin with a general introduction to backdoor threats, \n  outlining the attack assumptions, threat models, and real-world implications. It will then present a series of concrete attack and defense strategies \n  developed in recent research. The keynote will conclude with a forward-looking perspective on either the use of watermarking for model ownership \n  verification or the unique challenges posed by backdoor attacks in federated learning."
  },
  {
    "objectID": "index.html#call",
    "href": "index.html#call",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "Call for Talks/Posters",
    "text": "Call for Talks/Posters\nWe invite researchers, academics, and industry professionals to contribute to the workshop by presenting their work on adversarial threats and defenses for real-life learning systems.\n\nSubmission Types\nTalk Presentations (15-20 minutes) - Original research on adversarial ML, backdoor attacks, or defense mechanisms - Case studies from real-world deployments - Novel theoretical contributions to adversarial robustness\nPoster Presentations - Work-in-progress on adversarial ML security - Preliminary results and ongoing research - Demonstrations of tools and frameworks\n\n\nSubmission Guidelines\n\nPriority Criteria\nProposals will be evaluated based on:\n\nPublished Papers - Submissions based on peer-reviewed publications will receive priority consideration for talk slots\nRelevance - Direct relevance to adversarial threats on real-life systems\nNovelty - Original contributions to the field\nImpact - Practical implications for ML security\n\nPreferred Topics:\n\nAdversarial attacks on production ML systems\nBackdoor attacks and detection methods\n\nReal-world robustness evaluation\nDefense mechanisms for deployed models\nSecurity implications of ML in critical applications\n\n\n\n\nHow to Submit\nOption 1: Published Work\n\nProvide DOI, arXiv link, or full citation of your published paper\n\nOption 2: Abstract Submission\n\nSubmit an abstract (maximum 300 words)\nInclude preliminary results if available\nSpecify your presentation preference\n\n\n\nImportant Dates\n\n\nSubmission Deadline: July 18th, 2025\nNotification: July 25th, 2025\nFinal Program: August 1st, 2025\nWorkshop Date: September 17th, 2025\n\n\n\n\nSubmission Process\nSubmit your proposal through the registration form below by selecting “Talk proposal” or “Poster proposal” and completing the additional fields. All submissions will be reviewed by the organizing committee."
  },
  {
    "objectID": "index.html#registration",
    "href": "index.html#registration",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "Registration",
    "text": "Registration\n\nRegistration Information\n\n\nParticipation: Free of charge\nRegistration: Mandatory (Limited places available)\nConfirmation: You will receive a confirmation email\n\n\n\n\nOnline Registration Form\n\nRegistration Form\n\n  \n    \n      Nom (Last Name) *\n      \n    \n    \n      Prénom (First Name) *\n      \n    \n  \n  \n  \n    \n      Email *\n      \n    \n  \n  \n  \n    Affiliation/Institution *\n    \n  \n  \n  \n    \n      Position *\n      \n        -- Select Position --\n        Professor\n        Researcher\n        Post-doctoral Researcher\n        PhD Student\n        Master Student\n        Industry Professional\n        Other\n      \n    \n    \n      Country\n      \n    \n  \n  \n  \n  \n    Participation Type *\n    \n      \n        \n        Attendance only\n      \n      \n        \n        Talk proposal (15-20 minutes)\n      \n      \n        \n        Poster proposal\n      \n    \n  \n  \n  \n  \n    \n      Presentation Title\n      \n    \n\n    \n    \n      Abstract (max 300 words)\n      \n    \n    \n    \n      Published Paper Reference (if applicable)\n      \n      \n        Note: Proposals based on published papers will be given priority for talk slots.\n      \n    \n  \n  \n  \n    \n    I consent to the processing of my personal data for this event *"
  },
  {
    "objectID": "index.html#practical",
    "href": "index.html#practical",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "Practical Information",
    "text": "Practical Information\n\nVenue Details\n\nEsclangon Building\nCampus Pierre et Marie Curie\n4 place Jussieu, 75005 Paris\nFrance\nFloor: 1st floor\nCapacity: 40 participants\n\n\n\nGetting There\nLocation on Maps: - Google Maps - OpenStreetMap\nBy Public Transport: - Metro: Line 7, 10 (Jussieu station) - Direct access - RER: Line C (Saint-Michel Notre-Dame) - 5 min walk - Bus: Lines 63, 67, 86, 87 (Jussieu stop)\nBy Car: - Limited parking in the area - Nearby parking: Parking Maubert (5 min walk) - Parking locations on Google Maps"
  },
  {
    "objectID": "index.html#sponsors",
    "href": "index.html#sponsors",
    "title": "Adversarial Threats on Real Life Learning Systems",
    "section": "Sponsors & Partners",
    "text": "Sponsors & Partners\nWe thank our sponsors and partners for their support:\n\n  \n  \n  \n  \n  \n  \n\n\nThe workshop is supported by the Responsible AI Team.\nThe KINAITICS project is funded under Horizon Europe Grant Agreement n°101070176. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them."
  }
]